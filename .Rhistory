stackList = lapply(filesList, stack)
stackList = lapply(stackList, setNAs)
mosaic_bahrain = mosaic(stackList[[1]],stackList[[2]],stackList[[3]], stackList[[4]], fun = mean)
setwd("~/")
writeRaster(mosaic_bahrain, "training_unet/bahrain_2021.tif")
}
if(!file.exists("testarea_unet/doha_2021.tif")){
setwd("2021/files")
files = list.files(pattern = ".tif$")
filesList = as.list(files)
stackList = lapply(filesList, stack)
stackList = lapply(stackList, setNAs)
mosaic_bahrain = mosaic(stackList[[1]],stackList[[2]],fun = mean)
setwd("~/")
writeRaster(mosaic_bahrain, "testarea_unet/doha_2021.tif")
}
if(!file.exists("testarea_unet/doha_2019.tif")){
setwd("2019/files")
files = list.files(pattern = ".tif$")
filesList = as.list(files)
stackList = lapply(filesList, stack)
stackList = lapply(stackList, setNAs)
mosaic_bahrain = mosaic(stackList[[1]],stackList[[2]],fun = mean)
setwd("~/")
writeRaster(mosaic_bahrain, "testarea_unet/doha_2019.tif")
}
if(!file.exists("testarea_unet/doha_2017.tif")){
setwd("2017/files")
files = list.files(pattern = ".tif$")
filesList = as.list(files)
stackList = lapply(filesList, stack)
stackList = lapply(stackList, setNAs)
mosaic_bahrain = mosaic(stackList[[1]],stackList[[2]],fun = mean)
setwd("~/")
writeRaster(mosaic_bahrain, "testarea_unet/doha_2017.tif")
}
# load planet image of train area
st_bahrain = stack("training_unet/bahrain_2021.tif")
# load openstreetmap data of the streegts
streets_all = read_sf("data/shp/streets_osm.shp")
# transform osm data to crs of bahrain image
streets_utm = st_transform(streets_all, crs = crs(st_bahrain))
streets = st_crop(streets_utm, st_bbox(st_bahrain))
# creates a SpatVector object
v_lines = vect(streets)
# prere the output raster size
r = rast(v_lines, ncol = ncol(st_bahrain), nrow = nrow(st_bahrain))
x <- rasterize(v_lines, r, touches = TRUE, background = 0)
par(mfrow=c(1,2))
plot(x, col = c("white","black"), axes = FALSE, legend = FALSE)
# creates a SpatVector object
v_lines = vect(streets)
# prere the output raster size
r = rast(v_lines, ncol = ncol(st_bahrain), nrow = nrow(st_bahrain))
x <- rasterize(v_lines, r, touches = TRUE, background = 0)
par(mfrow=c(1,2))
plot(x, col = c("white","black"), axes = FALSE, legend = FALSE)
plotRGB(st_bahrain, r = 1, g=2, b=3)
primaries = streets[streets$fclass == "primary",]
secondaries = streets[streets$fclass == "secondary",]
tertiaries = streets[streets$fclass == "tertiary",]
trunks = streets[streets$fclass == "trunk",]
poly_streets = rbind(primaries,secondaries, tertiaries, trunks)
st_geometry(poly_streets[poly_streets$fclass == "trunk",]) = st_geometry(st_buffer(trunks, dist = 6))
st_geometry(poly_streets[poly_streets$fclass == "primary",]) = st_geometry(st_buffer(primaries, dist = 5))
st_geometry(poly_streets[poly_streets$fclass == "secondary",]) = st_geometry(st_buffer(secondaries, dist = 4))
st_geometry(poly_streets[poly_streets$fclass == "tertiary",]) = st_geometry(st_buffer(tertiaries, dist = 3))
v_poly = vect(poly_streets)
r_poly = rast(v_poly, ncol = ncol(st_bahrain), nrow = nrow(st_bahrain), extent = ext(x))
x_poly <- rasterize(v_poly, r_poly, touches = TRUE, background = 0)
par(mfrow=c(1,2))
plot(x_poly, col =c("white","black"), axes = FALSE, legend = FALSE)
plot(x, col =c("white","black"), axes = FALSE, legend = FALSE)
plotRGB(st_bahrain, r = 1, g=2, b=3)
result = max(x_poly,x)
writeRaster(result, "training_unet/street_mask.tif", overwrite = TRUE)
result = max(x_poly,x)
writeRaster(result, "training_unet/street_mask.tif", overwrite = TRUE)
dl_subsets <- function(inputrst, targetsize, targetdir, targetname="", img_info_only = FALSE, is_mask = FALSE){
require(jpeg)
require(raster)
#determine next number of quadrats in x and y direction, by simple rounding
targetsizeX <- targetsize[1]
targetsizeY <- targetsize[2]
inputX <- ncol(inputrst)
inputY <- nrow(inputrst)
#determine dimensions of raster so that
#it can be split by whole number of subsets (by shrinking it)
while(inputX%%targetsizeX!=0){
inputX = inputX-1
}
while(inputY%%targetsizeY!=0){
inputY = inputY-1
}
#determine difference
diffX <- ncol(inputrst)-inputX
diffY <- nrow(inputrst)-inputY
#determine new dimensions of raster and crop,
#cutting evenly on all sides if possible
newXmin <- floor(diffX/2)
newXmax <- ncol(inputrst)-ceiling(diffX/2)-1
newYmin <- floor(diffY/2)
newYmax <- nrow(inputrst)-ceiling(diffY/2)-1
rst_cropped <- suppressMessages(crop(inputrst, extent(inputrst,newYmin,newYmax,newXmin,newXmax)))
#writeRaster(rst_cropped,filename = target_dir_crop)
#return (list(ssizeX = ssizeX, ssizeY = ssizeY, nsx = nsx, nsy =nsy))
agg <- suppressMessages(aggregate(rst_cropped[[1]],c(targetsizeX,targetsizeY)))
agg[]    <- suppressMessages(1:ncell(agg))
agg_poly <- suppressMessages(rasterToPolygons(agg))
names(agg_poly) <- "polis"
pb <- txtProgressBar(min = 0, max = ncell(agg), style = 3)
for(i in 1:ncell(agg)) {
# rasterOptions(tmpdir=tmpdir)
setTxtProgressBar(pb, i)
e1  <- extent(agg_poly[agg_poly$polis==i,])
subs <- suppressMessages(crop(rst_cropped,e1))
#rescale to 0-1, for jpeg export
if(is_mask==FALSE){
subs <- suppressMessages((subs-cellStats(subs,"min"))/(cellStats(subs,"max")-cellStats(subs,"min")))
}
#write jpg
writeJPEG(as.array(subs),target = paste0(targetdir,targetname,i,".jpg"),quality = 1)
#writeRaster(subs,filename=paste0(targetdir,"SplitRas_",i,".tif"),overwrite=TRUE)
#return(c(extent(rst_cropped),crs(rst_cropped)))
}
close(pb)
#img_info <- list("tiles_rows"=nrow(rst_cropped)/targetsizeY, "tiles_cols"=ncol(rst_cropped)/targetsizeX,"crs"= crs(rst_cropped),"extent"=extent(rst_cropped))
#writeRaster(rst_cropped,filename = paste0(targetdir,"input_rst_cropped.tif"))
rm(subs,agg,agg_poly)
gc()
return(rst_cropped)
}
inputrst = stack("training_unet/bahrain_2021.tif")
inputrst_mask = raster("training_unet/street_mask.tif")
plotRGB(inputrst, r = 1, g=2, b=3)
plot(inputrst_mask, col=c("white","black"))
# Create subsets of Bahrain and Street mask
#dl_subsets(inputrst = inputrst, targetsize = c(448,448), targetdir = "training_unet/imgs/")
#dl_subsets(inputrst = inputrst_mask, targetsize = c(448,448), targetdir = "training_unet/masks/")
result_map <- raster("./testarea_unet/2019_out/mosaic.tif")%>%readAll()
hist(result_map, main='Street Preditiction Values in 2019')
result_map_0.3NA = result_map
result_map_0.3NA[result_map_0.3NA[[1]]<0.3] <- NA
# histogram
hist(values(result_map_0.3NA), main = "Classified Street Pixels in 2019")
# cell stats
cellStats(result_map, "sum")
cellStats(result_map, stat='mean', na.rm=TRUE, asSample=TRUE)
#standard deviation
# values > 0.3 -> 1, values <0.3 -> 0
# count numbers
result_map_0.3 =  result_map
result_map_0.3[result_map_0.3[[1]]<0.3] <- 0
result_map_0.3[result_map_0.3[[1]]>0.3] <- 1
cellStats(result_map_0.3, "sum")
viewRGB(input_img,layer.name = "input image", quantiles = c(0,1),r=1,g=2,b=3)+
#mapview(result_map,layer.name="street prediction", alpha.regions=0.4,na.alpha=0)
mapview(result_map, layer.name="Street Prediction in Doha in 2019", alpha.regions=1,na.alpha=0)
input_img <- stack("testarea_unet/doha_2019.tif")
viewRGB(input_img,layer.name = "input image", quantiles = c(0,1),r=1,g=2,b=3)+
#mapview(result_map,layer.name="street prediction", alpha.regions=0.4,na.alpha=0)
mapview(result_map, layer.name="Street Prediction in Doha in 2019", alpha.regions=1,na.alpha=0)
viewRGB(input_img,layer.name = "input image", quantiles = c(0,1),r=1,g=2,b=3)+
#mapview(result_map,layer.name="street prediction", alpha.regions=0.4,na.alpha=0)
mapview(result_map_0.3NA,layer.name="Street Prediction in Doha in 2019", alpha.regions=1,na.alpha=0)
plotRGB(input_img, main = "Test area", axes = TRUE,r=1,b=3)
train_img <- stack("Bahrain_2021_3Band/bahrain_2021.tif")
plotRGB(train_img, main = "Train area", axes = TRUE,r=1,b=3)
plot_layer_activations <- function(img_path, model, activations_layers,channels){
model_input_size <- c(model$input_shape[[2]], model$input_shape[[3]])
#preprocess image for the model
img <- image_load(img_path, target_size =  model_input_size) %>%
image_to_array() %>%
array_reshape(dim = c(1, model_input_size[1], model_input_size[2], 3)) %>%
imagenet_preprocess_input()
layer_outputs <- lapply(model$layers[activations_layers], function(layer) layer$output)
activation_model <- keras_model(inputs = model$input, outputs = layer_outputs)
activations <- predict(activation_model,img)
if(!is.list(activations)){
activations <- list(activations)
}
#function for plotting one channel of a layer, adopted from: Chollet (2018): "Deep learning with R"
plot_channel <- function(channel,layer_name,channel_name) {
rotate <- function(x) t(apply(x, 2, rev))
image(rotate(channel), axes = FALSE, asp = 1,
col = terrain.colors(12),main=paste("layer:",layer_name,"channel:",channel_name))
}
for (i in 1:length(activations)) {
layer_activation <- activations[[i]]
layer_name <- model$layers[[activations_layers[i]]]$name
n_features <- dim(layer_activation)[[4]]
for (c in channels){
channel_image <- layer_activation[1,,,c]
plot_channel(channel_image,layer_name,c)
}
}
}
par(mfrow=c(1,1))
plot(read_stars("./testarea_unet/2019_subsets/25.jpg"),rgb=c(1,2,3))
#visualize layers 3 and 10, channels 1 to 20
par(mfrow=c(3,4),mar=c(1,1,1,1),cex=0.5)
plot_layer_activations(img_path = "./testarea_unet/2019_subsets/25.jpg", model=pretrained_unet ,activations_layers = c(2,3,5,6,8,9,10,12,13,14), channels = 1:4)
library(keras)
library(tensorflow)
library(tfdatasets)
library(purrr)
library(ggplot2)
library(rsample)
library(stars)
library(raster)
library(reticulate)
library(mapview)
## we start with the "contratcing path"##
# input
input_tensor <- layer_input(shape = c(448,448,3))
#conv block 1
unet_tensor <- layer_conv_2d(input_tensor,filters = 64,kernel_size = c(3,3), padding = "same",activation = "relu")
conc_tensor2 <- layer_conv_2d(unet_tensor,filters = 64,kernel_size = c(3,3), padding = "same",activation = "relu")
unet_tensor <- layer_max_pooling_2d(conc_tensor2)
#conv block 2
unet_tensor <- layer_conv_2d(unet_tensor,filters = 128,kernel_size = c(3,3), padding = "same",activation = "relu")
conc_tensor1 <- layer_conv_2d(unet_tensor,filters = 128,kernel_size = c(3,3), padding = "same",activation = "relu")
unet_tensor <- layer_max_pooling_2d(conc_tensor1)
#"bottom curve" of unet
unet_tensor <- layer_conv_2d(unet_tensor,filters = 256,kernel_size = c(3,3), padding = "same",activation = "relu")
unet_tensor <- layer_conv_2d(unet_tensor,filters = 256,kernel_size = c(3,3), padding = "same",activation = "relu")
##  this is where the expanding path begins ##
# upsampling block 1
unet_tensor <- layer_conv_2d_transpose(unet_tensor,filters = 128,kernel_size = c(2,2),strides = 2,padding = "same")
unet_tensor <- layer_concatenate(list(conc_tensor1,unet_tensor))
unet_tensor <- layer_conv_2d(unet_tensor, filters = 128, kernel_size = c(3,3),padding = "same", activation = "relu")
unet_tensor <- layer_conv_2d(unet_tensor, filters = 128, kernel_size = c(3,3),padding = "same", activation = "relu")
# upsampling block 2
unet_tensor <- layer_conv_2d_transpose(unet_tensor,filters = 64,kernel_size = c(2,2),strides = 2,padding = "same")
unet_tensor <- layer_concatenate(list(conc_tensor2,unet_tensor))
unet_tensor <- layer_conv_2d(unet_tensor, filters = 64, kernel_size = c(3,3),padding = "same", activation = "relu")
unet_tensor <- layer_conv_2d(unet_tensor, filters = 64, kernel_size = c(3,3),padding = "same", activation = "relu")
# output
unet_tensor <- layer_conv_2d(unet_tensor,filters = 1,kernel_size = 1, activation = "sigmoid")
# combine final unet_tensor (carrying all the transformations applied through the layers)
# with input_tensor to create model
unet_model <- keras_model(inputs = input_tensor, outputs = unet_tensor)
## load pretrained vgg16 and use part of it as contracting path (feature extraction) ##
vgg16_feat_extr <- application_vgg16(weights = "imagenet", include_top = FALSE, input_shape = c (448,448,3))
# optionally freeze first layers to prevent changing of their weights, either whole convbase or only certain layers
# freeze_weights(vgg16_feat_extr) #or:
# freeze_weights(vgg16_feat_extr, to = "block1_pool")
# we'll not use the whole model but only up to layer 15
unet_tensor <- vgg16_feat_extr$layers[[15]]$output
## add the second part of 'U' for segemntation ##
# "bottom curve" of U-net
unet_tensor <- layer_conv_2d(unet_tensor, filters = 1024, kernel_size = 3, padding = "same", activation = "relu")
unet_tensor <- layer_conv_2d(unet_tensor, filters = 1024, kernel_size = 3, padding = "same", activation = "relu")
# upsampling block 1
unet_tensor <- layer_conv_2d_transpose(unet_tensor, filters = 512, kernel_size = 2, strides = 2, padding = "same")
unet_tensor <- layer_concatenate(list(vgg16_feat_extr$layers[[14]]$output, unet_tensor))
unet_tensor <- layer_conv_2d(unet_tensor, filters = 512, kernel_size = 3, padding = "same", activation = "relu")
unet_tensor <- layer_conv_2d(unet_tensor, filters = 512, kernel_size = 3, padding = "same", activation = "relu")
# upsampling block 2
unet_tensor <- layer_conv_2d_transpose(unet_tensor, filters = 256, kernel_size = 2, strides = 2, padding = "same")
unet_tensor <- layer_concatenate(list(vgg16_feat_extr$layers[[10]]$output, unet_tensor))
unet_tensor <- layer_conv_2d(unet_tensor,filters = 256, kernel_size = 3, padding = "same", activation = "relu")
unet_tensor <- layer_conv_2d(unet_tensor,filters = 256, kernel_size = 3, padding = "same", activation = "relu")
# upsampling block 3
unet_tensor <- layer_conv_2d_transpose(unet_tensor, filters = 128, kernel_size = 2, strides = 2, padding = "same")
unet_tensor <- layer_concatenate(list(vgg16_feat_extr$layers[[6]]$output, unet_tensor))
unet_tensor <- layer_conv_2d(unet_tensor, filters = 128, kernel_size = 3, padding = "same", activation = "relu")
unet_tensor <- layer_conv_2d(unet_tensor, filters = 128, kernel_size = 3, padding = "same", activation = "relu")
# upsampling block 4
unet_tensor <- layer_conv_2d_transpose(unet_tensor, filters = 64, kernel_size = 2, strides = 2, padding = "same")
unet_tensor <- layer_concatenate(list(vgg16_feat_extr$layers[[3]]$output, unet_tensor))
unet_tensor <- layer_conv_2d(unet_tensor, filters = 64, kernel_size = 3, padding = "same", activation = "relu")
unet_tensor <- layer_conv_2d(unet_tensor, filters = 64, kernel_size = 3, padding = "same", activation = "relu")
# final output
unet_tensor <- layer_conv_2d(unet_tensor, filters = 1, kernel_size = 1, activation = "sigmoid")
# create model from tensors
pretrained_unet <- keras_model(inputs = vgg16_feat_extr$input, outputs = unet_tensor)
spectral_augmentation <- function(img) {
img <- tf$image$random_brightness(img, max_delta = 0.3)
img <- tf$image$random_contrast(img, lower = 0.8, upper = 1.2)
img <- tf$image$random_saturation(img, lower = 0.8, upper = 1.2)
# make sure we still are between 0 and 1
img <- tf$clip_by_value(img,0, 1)
}
#adapted from: https://blogs.rstudio.com/ai/posts/2019-08-23-unet/ (accessed 2020-08-12)
dl_prepare_data <- function(files=NULL, train, predict=FALSE, subsets_path=NULL, model_input_shape = c(448,448), batch_size = 10L) {
if (!predict){
#function for random change of saturation,brightness and hue,
#will be used as part of the augmentation
spectral_augmentation <- function(img) {
img <- tf$image$random_brightness(img, max_delta = 0.3)
img <- tf$image$random_contrast(img, lower = 0.8, upper = 1.1)
img <- tf$image$random_saturation(img, lower = 0.8, upper = 1.1)
# make sure we still are between 0 and 1
img <- tf$clip_by_value(img, 0, 1)
}
#create a tf_dataset from the input data.frame
#right now still containing only paths to images
dataset <- tensor_slices_dataset(files)
#use dataset_map to apply function on each record of the dataset
#(each record being a list with two items: img and mask), the
#function is list_modify, which modifies the list items
#'img' and 'mask' by using the results of applying decode_jpg on the img and the mask
#-> i.e. jpgs are loaded and placed where the paths to the files were (for each record in dataset)
dataset <-
dataset_map(dataset, function(.x)
list_modify(.x,img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),
mask = tf$image$decode_jpeg(tf$io$read_file(.x$mask))))
#convert to float32:
#for each record in dataset, both its list items are modyfied
#by the result of applying convert_image_dtype to them
dataset <-
dataset_map(dataset, function(.x)
list_modify(.x, img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),
mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)))
#resize:
#for each record in dataset, both its list items are modified
#by the results of applying resize to them
dataset <-
dataset_map(dataset, function(.x)
list_modify(.x, img = tf$image$resize(.x$img, size = shape(model_input_shape[1], model_input_shape[2])),
mask = tf$image$resize(.x$mask, size = shape(model_input_shape[1], model_input_shape[2]))))
# data augmentation performed on training set only
if (train) {
#augmentation 1: flip left right, including random change of
#saturation, brightness and contrast
#for each record in dataset, only the img item is modified by the result
#of applying spectral_augmentation to it
augmentation <-
dataset_map(dataset, function(.x)
list_modify(.x, img = spectral_augmentation(.x$img)))
#...as opposed to this, flipping is applied to img and mask of each record
augmentation <-
dataset_map(augmentation, function(.x)
list_modify(.x, img = tf$image$flip_left_right(.x$img),
mask = tf$image$flip_left_right(.x$mask)))
dataset_augmented <- dataset_concatenate(dataset,augmentation)
#augmentation 2: flip up down,
#including random change of saturation, brightness and contrast
augmentation <-
dataset_map(dataset, function(.x)
list_modify(.x, img = spectral_augmentation(.x$img)))
augmentation <-
dataset_map(augmentation, function(.x)
list_modify(.x, img = tf$image$flip_up_down(.x$img),
mask = tf$image$flip_up_down(.x$mask)))
dataset_augmented <- dataset_concatenate(dataset_augmented,augmentation)
#augmentation 3: flip left right AND up down,
#including random change of saturation, brightness and contrast
augmentation <-
dataset_map(dataset, function(.x)
list_modify(.x, img = spectral_augmentation(.x$img)))
augmentation <-
dataset_map(augmentation, function(.x)
list_modify(.x, img = tf$image$flip_left_right(.x$img),
mask = tf$image$flip_left_right(.x$mask)))
augmentation <-
dataset_map(augmentation, function(.x)
list_modify(.x, img = tf$image$flip_up_down(.x$img),
mask = tf$image$flip_up_down(.x$mask)))
dataset_augmented <- dataset_concatenate(dataset_augmented,augmentation)
}
# shuffling on training set only
if (train) {
dataset <- dataset_shuffle(dataset_augmented, buffer_size = batch_size*128)
}
# train in batches; batch size might need to be adapted depending on
# available memory
dataset <- dataset_batch(dataset, batch_size)
# output needs to be unnamed
dataset <-  dataset_map(dataset, unname)
}else{
#make sure subsets are read in in correct order
#so that they can later be reassembled correctly
#needs files to be named accordingly (only number)
o <- order(as.numeric(tools::file_path_sans_ext(basename(list.files(subsets_path)))))
subset_list <- list.files(subsets_path, full.names = T)[o]
dataset <- tensor_slices_dataset(subset_list)
dataset <-
dataset_map(dataset, function(.x)
tf$image$decode_jpeg(tf$io$read_file(.x)))
dataset <-
dataset_map(dataset, function(.x)
tf$image$convert_image_dtype(.x, dtype = tf$float32))
dataset <-
dataset_map(dataset, function(.x)
tf$image$resize(.x, size = shape(model_input_shape[1], model_input_shape[2])))
dataset <- dataset_batch(dataset, batch_size)
dataset <-  dataset_map(dataset, unname)
}
}
#get paths
files <- data.frame(
img = list.files("training_unet/imgs/", full.names = TRUE, pattern = "*.jpg"),
mask = list.files("training_unet/masks/", full.names = TRUE, pattern = "*.jpg")
)
# split the data into training and validation datasets.
files <- initial_split(files, prop = 0.7)
# prepare data for training
training_dataset <- dl_prepare_data(training(files),train = TRUE,model_input_shape = c(448,448),batch_size = 10L)
validation_dataset <- dl_prepare_data(testing(files),train = FALSE,model_input_shape = c(448,448),batch_size = 10L)
# get all tensors through the python iterator
training_tensors <- training_dataset%>%as_iterator()%>%iterate()
#how many tensors?
length(training_tensors)
#now whatÂ´s the shape of one tensor, here the image tensor (list item 1) of the first batch?
training_tensors[[1]][[1]]$shape
# and the mask
training_tensors[[1]][[2]]$shape
#how about the last batch?
training_tensors[[8]][[1]]$shape
training_tensors[[8]][[2]]$shape
compile(
pretrained_unet,
optimizer = optimizer_rmsprop(lr = 1e-5),
loss = "binary_crossentropy",
metrics = c(metric_binary_accuracy)
)
diagnostics <- fit(pretrained_unet,
training_dataset,
epochs = 5,
validation_data = validation_dataset)
plot(diagnostics)
save_model_hdf5(pretrained_unet,filepath = "./pretrained_unet.h5")
gc()
library(raster)
result_map_21 <- raster("./testarea_unet/2021_out/mosaic.tif")%>%readAll()
library(dplyr)
library(keras)
library(tensorflow)
library(tfdatasets)
library(purrr)
library(ggplot2)
library(rsample)
library(stars)
library(terra)
result_map_21 <- raster("./testarea_unet/2021_out/mosaic.tif")%>%readAll()
result_map_21_0.3NA = result_map_21
result_map_21_0.3NA[result_map_21_0.3NA[[1]]<0.3] <- NA
hist(values(result_map_21_0.3NA), main = "Classified Street Pixels comparison",col = rgb(0, 1, 0, 0.5),xlab="street probability")
result_map_17 <- raster("./testarea_unet/2017_out/mosaic.tif")%>%readAll()
result_map_17_0.3NA = result_map_17
result_map_17_0.3NA[result_map_17_0.3NA[[1]]<0.3] <- NA
hist(values(result_map_17_0.3NA),col = rgb(0, 0, 1, 0.5), add = TRUE)
result_map_19 <- raster("./testarea_unet/2019_out/mosaic.tif")%>%readAll()
result_map_19_0.3NA = result_map_19
result_map_19_0.3NA[result_map_19_0.3NA[[1]]<0.3] <- NA
hist(values(result_map_19_0.3NA),col = rgb(1, 0, 0, 0.5), add = TRUE)
legend("topright", inset=.02, title="Year",
c("2017","2019","2021"), fill=c(rgb(0, 0, 1, 0.5),rgb(1, 0, 0, 0.5),rgb(0, 1, 0, 0.5)), horiz=TRUE, cex=0.8)
result_map2017 <- raster("./testarea_unet/2017_out/mosaic.tif")%>%readAll()
result_map2019 <- raster("./testarea_unet/2019_out/mosaic.tif")%>%readAll()
result_map2021 <- raster("./testarea_unet/2021_out/mosaic.tif")%>%readAll()
viewRGB(input_img,layer.name = "input image", quantiles = c(0,1),r=1,g=2,b=3)+
#mapview(result_map,layer.name="street prediction", alpha.regions=0.4,na.alpha=0)
mapview(result_map2017,layer.name="street prediction in Doha 2017", alpha.regions=1,na.alpha=0)+
mapview(result_map2019,layer.name="street prediction in Doha 2019", alpha.regions=1,na.alpha=0)+
mapview(result_map2021,layer.name="street prediction in Doha 2021", alpha.regions=1,na.alpha=0)
library(mapview)
result_map2017 <- raster("./testarea_unet/2017_out/mosaic.tif")%>%readAll()
result_map2019 <- raster("./testarea_unet/2019_out/mosaic.tif")%>%readAll()
result_map2021 <- raster("./testarea_unet/2021_out/mosaic.tif")%>%readAll()
viewRGB(input_img,layer.name = "input image", quantiles = c(0,1),r=1,g=2,b=3)+
#mapview(result_map,layer.name="street prediction", alpha.regions=0.4,na.alpha=0)
mapview(result_map2017,layer.name="street prediction in Doha 2017", alpha.regions=1,na.alpha=0)+
mapview(result_map2019,layer.name="street prediction in Doha 2019", alpha.regions=1,na.alpha=0)+
mapview(result_map2021,layer.name="street prediction in Doha 2021", alpha.regions=1,na.alpha=0)
result_map2017 <- raster("./testarea_unet/2017_out/mosaic.tif")%>%readAll()
result_map2019 <- raster("./testarea_unet/2019_out/mosaic.tif")%>%readAll()
result_map2021 <- raster("./testarea_unet/2021_out/mosaic.tif")%>%readAll()
#viewRGB(input_img,layer.name = "input image", quantiles = c(0,1),r=1,g=2,b=3)+
#mapview(result_map,layer.name="street prediction", alpha.regions=0.4,na.alpha=0)
mapview(result_map2017,layer.name="street prediction in Doha 2017", alpha.regions=1,na.alpha=0)+
mapview(result_map2019,layer.name="street prediction in Doha 2019", alpha.regions=1,na.alpha=0)+
mapview(result_map2021,layer.name="street prediction in Doha 2021", alpha.regions=1,na.alpha=0)
# compare pixel sums
result_map_2021_binary
plot(result_map2017, main = "2017")
plot(result_map2019, main = "2019")
plot(result_map2021, main = "2021")
plot(result_map2021- result_map2017, main = "2017 to 2021")
plot(result_map2021- result_map2019, main = "2019 to 2021")
result_map_2017_binary = result_map2017
result_map_2017_binary[result_map_2017_binary>=0.3] = 1
result_map_2017_binary[result_map_2017_binary<0.3] = 0
result_map_2019_binary = result_map2019
result_map_2019_binary[result_map_2019_binary>=0.3] = 1
result_map_2019_binary[result_map_2019_binary<0.3] = 0
result_map_2021_binary = result_map2021
result_map_2021_binary[result_map_2021_binary>=0.3] = 1
result_map_2021_binary[result_map_2021_binary<0.3] = 0
plot(result_map_2017_binary, main = "2017", legend = FALSE)
plot(result_map_2019_binary, main = "2019", legend = FALSE)
plot(result_map_2021_binary, main = "2021", legend = FALSE)
plot(result_map_2021_binary - result_map_2017_binary, main="2017 - 2021", legend = FALSE)
plot(result_map_2021_binary - result_map_2019_binary, main="2019 - 2021", legend = FALSE)
plot(result_map_2019_binary - result_map_2017_binary, main="2017 - 2019", legend = FALSE)
# compare pixel sums
result_map_2021_binary
sum(values(result_map2021_binary))
count(result_map2021_binary)
count(result_map_2021_binary)
sum(result_map_2021_binary)
values(result_map_2021_binary)
sum(values(result_map_2021_binary))
sum(values(result_map_2017_binary))
sum(values(result_map_2019_binary))
sum(values(result_map_2021_binary)) / sum(values(result_map_2017_binary))
sum(values(result_map_2019_binary)) / sum(values(result_map_2017_binary))
result_map_2019_binary
1792*3
3136*3
